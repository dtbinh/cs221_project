t  lang and m  toussaint 2010 planning with noisy probabilistic relational rules volume 39 pages 149

noisy probabilistic relational rules are a promising world model representation for several reasons they are compact and generalize over world instantiations they are usually interpretable and they can be learned effectively from the action experiences in complex worlds we investigate reasoning with such rules in grounded relational domains our algorithms exploit the compactness of rules for efficient and flexible decisiontheoretic planning as a first approach we combine these rules with the upper confidence bounds applied to trees uct algorithm based on lookahead trees our second approach converts these rules into a structured dynamic bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states we evaluate the effectiveness of our approaches for planning in a simulated complex 3d robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition empirical results show that our methods can solve problems where existing methods fail



m  katz and c  domshlak 2010 implicit abstraction heuristics volume 39 pages 51126

statespace search with explicit abstraction heuristics is at the state of the art of costoptimal planning these heuristics are inherently limited nonetheless because the size of the abstract space must be bounded by some even if a very large constant targeting this shortcoming we introduce the notion of iadditive implicit abstractionsi in which the planning task is abstracted by instances of tractable fragments of optimal planning we then introduce a concrete setting of this framework called iforkdecompositioni that is based on two novel fragments of tractable costoptimal planning the induced admissible heuristics are then studied formally and empirically this  study testifies for the accuracy of the fork decomposition heuristics yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them indeed some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining ihsi for each evaluated state isi by a very fast lookup in a database by contrast while forkdecomposition heuristics can be calculated in polynomial time  computing them is far from being fast to address this problem we show that the timepernode complexity bottleneck of the forkdecomposition heuristics  can be successfully overcome we demonstrate that an equivalent of the explicit abstraction notion of a database exists for the forkdecomposition abstractions as well despite their exponentialsize abstract spaces we then verify empirically that heuristic search with the databased forkdecomposition heuristics favorably competes with the state of the art of costoptimal planning



s  richter and m  westphal 2010 the lama planner guiding costbased anytime planning with landmarks volume 39 pages 127177

lama is a classical planning system based on heuristic forward search its core feature is the use of a pseudoheuristic derived from landmarks propositional formulas that must be true in every solution of a planning task lama builds on the fast downward planning system using finitedomain rather than binary state variables and multiheuristic search the latter is employed to combine the landmark heuristic with a variant of the wellknown ff heuristic both heuristics are costsensitive focusing on highquality solutions in the case where actions have nonuniform cost a weighted a search is used with iteratively decreasing weights so that the planner continues to search for plans of better quality until the search is terminated
lama showed best performance among all planners in the sequential satisficing track of the international planning competition 2008 in this paper we present the system in detail and investigate which features of lama are crucial for its performance we present individual results for some of the domains used at the competition demonstrating good and bad cases for the techniques implemented in lama overall we find that using landmarks improves performance whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial we show that in some domains a search that ignores cost solves far more problems raising the question of how to deal with action costs more effectively in the future the iterated weighted a search greatly improves results and shows synergy effects with the use of landmarks



g  chalkiadakis e  elkind e  markakis m  polukarov and n  r jennings 2010 cooperative games with overlapping coalitions volume 39 pages 179216

in the usual models of cooperative game theory the outcome of a coalition formation process is either the grand coalition or a coalition structure that consists of disjoint coalitions however in many domains where coalitions are associated with tasks an agent may be involved in executing more than one task and thus may distribute his resources among several coalitions to tackle such scenarios we introduce a model for cooperative games with overlapping coalitionsor overlapping coalition formation ocf games we then explore the issue of stability in this setting in particular we introduce a notion of the core which generalizes the corresponding notion in the traditional nonoverlapping scenario then under some quite general conditions we characterize the elements of the core and show that any element of the core maximizes the social welfare we also introduce a concept of balancedness for overlapping coalitional games and use it to characterize coalition structures that can be extended to elements of the core finally we generalize the notion of convexity to our setting and show that under some natural assumptions convex games have a nonempty core moreover we introduce two alternative notions of stability in ocf that allow a wider range of deviations and explore the relationships among the corresponding definitions of the core as well as the classic nonoverlapping core and the aubin core we illustrate the general properties of the three cores and also study them from a computational perspective thus obtaining additional insights into their fundamental structure



m  o riedl and r  m young 2010 narrative planning balancing plot and character volume 39 pages 217268

narrative and in particular storytelling is an important part of the human experience  consequently computational systems that can reason about narrative can be more effective communicators entertainers educators and trainers  one of the central challenges in computational narrative reasoning is narrative generation the automated creation of meaningful event sequences  there are many factors  logical and aesthetic  that contribute to the success of a narrative artifact  central to this success is its understandability  we argue that the following two attributes of narratives are universal a the logical causal progression of plot and b character believability  character believability is the perception by the audience that the actions performed by characters do not negatively impact the audiences suspension of disbelief  specifically characters must be perceived by the audience to be intentional agents  in this article we explore the use of refinement search as a technique for solving the narrative generation problem  to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold we describe a novel refinement search planning algorithm  the intentbased partial order causal link ipocl planner  that in addition to creating causally sound plot progression reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals we present the results of an empirical evaluation that demonstrates that narrative plans generated by the ipocl algorithm support audience comprehension of character intentions better than plans generated by conventional partialorder planners



v  bulitko y  bj246rnsson and r  lawrence 2010 casebased subgoaling in realtime heuristic search for video game pathfinding volume 39 pages 269300

realtime heuristic search algorithms satisfy a constant bound on the amount of planning per action independent of problem size as a result they scale up well as problems become larger this property would make them well suited for video games where artificial intelligence controlled agents must react quickly to user commands and to other agents actions on the downside realtime search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by revisiting the same problem states repeatedly the situation changed recently with a new algorithm d lrta which attempted to eliminate learning by automatically selecting subgoals d lrta is well poised for video games except it has a complex and memorydemanding precomputation phase during which it builds a database of subgoals in this paper we propose a simpler and more memoryefficient way of precomputing subgoals thereby eliminating the main obstacle to applying stateoftheart realtime search methods in video games the new algorithm solves a number of randomly chosen problems offline compresses the solutions into a series of subgoals and stores them in a database when presented with a novel problem online it queries the database for the most similar previously solved case and uses its subgoals to solve the problem in the domain of pathfinding on four large video game maps the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14 less precomputation time




a  feldman g  provan and a  van gemund 2010 a modelbased active testing approach to sequential diagnosis volume 39 pages 301334

modelbased diagnostic reasoning often leads to a large number of diagnostic hypotheses the set of diagnoses can be reduced by taking into account extra observations passive monitoring measuring additional variables probing or executing additional tests sequential diagnosistest sequencing in this paper we combine the above approaches with techniques from automated test pattern generation atpg and modelbased diagnosis mbd into a framework called fractal framework for active testing algorithms apart from the inputs and outputs that connect a system to its environment in active testing we consider additional input variables to which a sequence of test vectors can be supplied we address the computationally hard problem of computing optimal control assignments as defined in fractal in terms of a greedy approximation algorithm called fractalg we compare the decrease in the number of remaining minimal cardinality diagnoses of fractalg to that of two more fractal algorithms fractalatpg and fractalp fractalatpg is based on atpg and sequential diagnosis while fractalp is based on probing and although not an active testing algorithm provides a baseline for comparing the lower bound on the number of reachable diagnoses for the fractal algorithms we empirically evaluate the tradeoffs of the three fractal algorithms by performing extensive experimentation on the iscas8574xxx benchmark of combinational circuits



b  bidyuk r  dechter and e  rollon 2010 active tuplesbased scheme for bounding posterior beliefs volume 39 pages 335371

the paper presents a scheme for computing lower and upper bounds on the posterior marginals in bayesian networks with discrete variables its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner the scheme uses the cutset conditioning principle to tighten existing bounding schemes  and to facilitate anytime behavior utilizing  a fixed  number of cutset tuples the accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time we demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plugin scheme



b  banerjee and b  chandrasekaran 2010 a constraint satisfaction framework for executing perceptions and actions in diagrammatic reasoning volume 39 pages 373427

diagrammatic reasoning dr is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on languagelike representations the research reported in this paper is a contribution to building a general purpose dr system as an extension to a soarlike problem solving architecture the work is in a framework in which dr is modeled as a process where subtasks are solved as appropriate either by inference from symbolic representations or by interaction with a diagram ie perceiving specified information from a diagram or modifyingcreating objects in a diagram in specified ways according to problem solving needs the perceptions and actions in most dr systems built so far are handcoded for the specific application even when the rest of the system is built using the general architecture the absence of a general framework for executing perceptionsactions poses as a major hindrance to using them opportunistically  the essence of openended search in problem solving
our goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasksdomains without human intervention we observe that the domaintaskspecific visual perceptionsactions can be transformed into domaintaskindependent spatial problems we specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an openended vocabulary of properties relations and actions involving three kinds of diagrammatic objects  points curves regions solving a spatial problem from this specification requires computing the equivalent simplified quantifierfree expression the complexity of which is inherently doubly exponential we represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems we show that if the symbolic solution to a subproblem can be expressed concisely quantifiers can be eliminated from spatial problems in loworder polynomial time using similar previously solved subproblems this requires determining the similarity of two problems the existence of a mapping between them computable in polynomial time and designing a memory for storing previously solved problems so as to facilitate search the efficacy of the idea is shown by time complexity analysis we demonstrate the proposed approach by executing perceptions and actions involved in dr tasks in two army applications



s  rudolph and b  glimm 2010 nominals inverses counting and conjunctive queries or why infinity is your friend volume 39 pages 429481

description logics are knowledge representation formalisms that provide for example the logical underpinning of the w3c owl standards conjunctive queries the standard query language in databases have recently gained significant attention as an expressive formalism for querying description logic knowledge bases several different techniques for deciding conjunctive query entailment are available for a wide range of dls nevertheless the combination of nominals inverse roles and number restrictions in owl 1 and owl 2 dl causes unsolvable problems for the techniques hitherto available we tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the dl alchoiqb that contains all three problematic constructors simultaneously provided that queries contain only simple roles our result also shows decidability of entailment of unions of conjunctive queries in the logic that underpins owl 1 dl and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the description logics underlying the owl standards



m  geist and o  pietquin 2010 kalman temporal differences volume 39 pages 483532

because reinforcement learning suffers from a lack of scalability online value and q function approximation has received increasing interest this last decade this contribution introduces a novel approximation scheme namely the kalman temporal differences ktd framework that exhibits the following features sampleefficiency nonlinear approximation nonstationarity handling and uncertainty management a first ktdbased algorithm is provided for deterministic markov decision processes mdp which produces biased estimates in the case of stochastic transitions than the extended ktd framework xktd solving stochastic mdp is described convergence is analyzed for special cases for both deterministic and stochastic transitions related algorithms are experimented on classical benchmarks they compare favorably to the state of the art while exhibiting the announced features



k   daniel a  nash s  koenig and a  felner 2010 theta anyangle path planning on grids volume 39 pages 533579

  grids with blocked and unblocked cells are often used to represent terrain in robotics and video games however paths formed by grid edges can be longer than true shortest paths in the terrain since their headings are artificially constrained we present two new correct and complete anyangle pathplanning algorithms that avoid this shortcoming  basic theta and anglepropagation theta are both variants of a that propagate information along grid edges without constraining paths to grid edges basic theta is simple to understand and implement fast and finds short paths however it is not guaranteed to find true shortest paths anglepropagation theta achieves a better worstcase complexity per vertex expansion than basic theta by propagating angle ranges when it expands vertices but is more complex not as fast and finds slightly longer paths we refer to basic theta and anglepropagation theta collectively as theta theta has unique properties which we analyze in detail we show experimentally that it finds shorter paths than both a with postsmoothed paths and field d the only other version of a we know of that propagates information along grid edges without constraining paths to grid edges with a runtime comparable to that of a on grids finally we extend theta to grids that contain unblocked cells with nonuniform traversal costs and introduce variants of theta which provide different tradeoffs between path length and runtime



s  dasgupta and v  ng 2010 which clustering do you want inducing your ideal clustering with minimal feedback volume 39 pages 581632

while traditional research on text clustering has largely focused on grouping documents by topic it is conceivable that a user may want to cluster documents along other dimensions such as the authors mood gender age or sentiment without knowing the users intention a clustering algorithm will only group documents along the most prominent dimension which may not be the one the user desires to address the problem of clustering documents along the userdesired dimension previous work has focused on learning a similarity metric from data manually annotated with the users intention or having a human construct a feature space in an interactive manner during the clustering process with the goal of reducing reliance on human knowledge for finetuning the similarity function or selecting the relevant features required by these approaches we propose a novel active clustering algorithm which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words we demonstrate the viability of our algorithm on a variety of commonlyused sentiment datasets



a  krause and e  horvitz 2010 a utilitytheoretic approach to privacy in online services volume 39 pages 633662

online offerings such as web search news portals and ecommerce applications face the challenge of providing highquality service to a large heterogeneous user base recent efforts have highlighted the potential to improve performance by introducing methods to personalize services based on special knowledge about users and their context for example a users demographics location and past search and browsing may be useful in enhancing the results offered in response to web search queries  however reasonable concerns about privacy by both users providers and government agencies acting on behalf of citizens may limit access by services to such information   we introduce and explore an economics of privacy in personalization where people can opt to share personal information in a standing or ondemand manner in return for expected enhancements in the quality of an online service we focus on the example of web search and formulate realistic objective functions for search efficacy and privacy we demonstrate how we can find a provably nearoptimal optimization of the utilityprivacy tradeoff in an efficient manner  we evaluate our methodology on data drawn from a log of the search activity of volunteer participants we separately assess users preferences about privacy and utility via a largescale survey aimed at eliciting preferences about peoples willingness to trade the sharing of personal data in returns for gains in search efficiency we show that a significant level of personalization can be achieved using a relatively small amount of information about users



g  j228ger and w  zhang 2010 an effective algorithm for and phase transitions of the directed hamiltonian cycle problem volume 39 pages 663687

the hamiltonian cycle problem hcp is an important combinatorial problem with applications in many areas it is among the first  problems used for studying intrinsic properties including phase transitions  of combinatorial problems while thorough theoretical and experimental analyses have been made on the hcp in undirected graphs a limited  amount of work has been done for the hcp in directed graphs dhcp 




e  burns s  lemons w  ruml and r  zhou 2010 bestfirst heuristic search for multicore machines  volume 39 pages 689743

to harness modern multicore processors it is imperative to develop parallel versions of fundamental algorithms in this paper we compare different approaches to parallel bestfirst search in a sharedmemory setting we present a new method pbnf that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking pbnf allows speculative expansions when necessary to keep threads busy we identify and fix potential livelock conditions in our approach proving its correctness using temporal logic our approach is general allowing it to extend easily to suboptimal and anytime heuristic search in an empirical comparison on strips planning grid pathfinding and sliding tile puzzle problems using 8core machines we show that a weighted a and anytime weighted a implemented using pbnf yield faster search than improved versions of previous parallel search proposals




j  xu and c  r shelton 2010 intrusion detection using continuous time bayesian networks volume 39 pages 745774

intrusion detection systems idss fall into two highlevel categories networkbased systems nids that monitor network behaviors and hostbased systems hids that monitor system calls in this work we present a general technique for both systems we use anomaly detection which identifies patterns not conforming to a historic norm in both types of systems the rates of change vary dramatically over time due to burstiness and over components due to service difference to efficiently model such systems we use continuous time bayesian networks ctbns and avoid specifying a fixed update interval common to discretetime models we build generative models from the normal training data and abnormal behaviors are flagged based on their likelihood under this norm for nids we construct a hierarchical ctbn model for the network packet traces and use raoblackwellized particle filtering to learn the parameters we illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces the mawi dataset and the lbnl dataset for hids we develop a novel learning method to deal with the finite resolution of system log file time stamps without losing the benefits of our continuous time model we demonstrate the method by detecting intrusions in the darpa 1998 bsm dataset
