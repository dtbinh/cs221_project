y  chali s  r joty and s  a hasan 2009 complex question answering unsupervised learning approaches and experiments volume 35 pages 147

complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented informative multidocument summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information in this paper we experiment with one empirical method and two unsupervised statistical machine learning techniques kmeans and expectation maximization em for computing relative importance of the sentences we compare the results of these approaches our experiments show that the empirical approach outperforms the other two techniques and em performs better than kmeans however the performance of these approaches depends entirely on the feature set used and the weighting of these features in order to measure the importance and relevance to the user query we extract different kinds of features ie lexical lexical semantic cosine similarity basic element tree kernel based syntactic and shallowsemantic for each of the document sentences we use a local search technique to learn the weights of the features to the best of our knowledge no study has used tree kernel functions to encode syntacticsemantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate queryfocused summaries or answers to complex questions for each of our methods of generating summaries ie empirical kmeans and em we show the effects of syntactic and shallowsemantic features over the bagofwords bow features



j  hoffmann p  bertoli m  helmert and m  pistore 2009 messagebased web service composition integrity constraints and planning under uncertainty a new connection volume 35 pages 49117

thanks to recent advances ai planning has become the underlying technique for several applications figuring prominently among these is automated web service composition wsc at the capability level where services are described in terms of preconditions and effects over ontological concepts a key issue in addressing wsc as planning is that ontologies are not only formal vocabularies they also axiomatize the possible relationships between concepts such axioms correspond to what has been termed integrity constraints in the actions and change literature and applying a web service is essentially a belief update operation the reasoning required for belief update is known to be harder than reasoning in the ontology itself the support for belief update is severely limited in current planning tools




s  d ramchurn c  mezzetti a  giovannucci j  a rodriguezaguilar r  k dash and n  r jennings 2009 trustbased mechanisms for robust and efficient task allocation in the presence of execution uncertainty volume 35 pages 119159

vickreyclarkegroves vcg mechanisms are often used to allocate tasks to selfish and rational agents vcg mechanisms are  incentive compatible direct mechanisms that are efficient ie maximise social utility and individually rational ie agents prefer to join rather than opt out however an important assumption of these mechanisms is that the agents will always successfully complete their allocated tasks clearly this assumption is unrealistic in many realworld applications where agents can and  often do fail in their endeavours moreover whether an agent is deemed to have failed may be perceived differently by different agents such subjective perceptions about an agents probability of succeeding at a given task are often captured and reasoned about using the notion of trust given this background in this paper we investigate the design of novel mechanisms that take into account the  trust between agents when allocating tasks  




v  conitzer 2009 eliciting singlepeaked preferences using comparison queries volume 35 pages 161191

voting is a general method for aggregating the preferences of multiple agents  each agent ranks all the possible alternatives and based on this an aggregate ranking of the alternatives or at least a winning alternative is produced  however when there are many alternatives it is impractical to simply ask agents to report their complete preferences rather the agents preferences or at least the relevant parts thereof need to be elicited  this is done by asking the agents a hopefully small number of simple queries about their preferences such as comparison queries which ask an agent to compare two of the alternatives  prior work on preference elicitation in voting has focused on the case of unrestricted preferences  it has been shown that in this setting it is sometimes necessary to ask each agent almost as many queries as would be required to determine an arbitrary ranking of the alternatives  in contrast in this paper we focus on singlepeaked preferences  we show that such preferences can be elicited using only a linear number of comparison queries if either the order with respect to which preferences are singlepeaked is known or at least one other agents complete preferences are known  we show that using a sublinear number of queries does not suffice  we also consider the case of cardinally singlepeaked preferences  for this case we show that if the alternatives cardinal positions are known then an agents preferences can be elicited using only a logarithmic number of queries however we also show that if the cardinal positions are not known then a sublinear number of queries does not suffice  we present experimental results for all elicitation algorithms  we also consider the problem of only eliciting enough information to determine the aggregate ranking and show that even for this more modest objective a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions  finally we discuss whether and how these techniques can be applied when preferences are almost singlepeaked



r  elyaniv and d  pechyony 2009 transductive rademacher complexity and its applications volume 35 pages 193234

we develop a technique for deriving datadependent error bounds for transductive learning algorithms based on transductive rademacher complexity our technique is based on a novel general error bound for transduction in terms of transductive rademacher complexity together with a novel bounding technique for rademacher averages for particular algorithms in terms of their unlabeledlabeled representation this technique is relevant to many advanced graphbased transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms finally we present a new pacbayesian bound for mixtures of transductive algorithms based on our rademacher bounds



m  petrik and s  zilberstein 2009 a bilinear programming approach for multiagent planning volume 35 pages 235274

multiagent planning and coordination problems are common and known to be computationally hard  we show that a wide range of twoagent problems can be formulated as bilinear programs  we present a successive approximation algorithm that significantly outperforms the coverage set algorithm which is the stateoftheart method for this class of multiagent problems because the algorithm is formulated for bilinear programs it is more general and simpler to implement the new algorithm can be terminated at any time andunlike the coverage set algorithmit facilitates the derivation of a useful online performance bound it is also much more efficient on average reducing the computation time of the optimal solution by about four orders of magnitude  finally we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs



p  faliszewski e  hemaspaandra l  a hemaspaandra and j  rothe 2009 llull and copeland voting computationally resist bribery and constructive control volume 35 pages 275341

control and bribery are settings in which an external agent seeks to influence the outcome of an election  constructive control of elections refers to attempts by an agent to via such actions as additiondeletionpartition of candidates or voters ensure that a given candidate wins  destructive control refers to attempts by an agent to via the same actions preclude a given candidates victory an election system in which an agent can sometimes affect the result and it can be determined in polynomial time on which inputs the agent can succeed is said to be vulnerable to the given type of control  an election system in which an agent can sometimes affect the result yet in which it is nphard to recognize the inputs on which the agent can succeed is said to be resistant to the given type of control




r  sebastiani and m  vescovi 2009 automated reasoning in modal and description logics via sat encoding the case study of kmalcsatisfiability volume 35 pages 343389

in the last two decades modal and description logics have been applied to numerous areas of computer science including knowledge representation formal verification database theory distributed computing and more recently semantic web and ontologies for this reason the problem of automated reasoning in modal and description logics has been thoroughly investigated in particular many approaches have been proposed for efficiently handling the satisfiability of the core normal modal logic km and of its notational variant the description logic alc although simple in structure kmalc is computationally very hard to reason on its satisfiability being pspacecomplete  
in this paper we start exploring the idea of performing automated reasoning tasks in modal and description logics by encoding them into sat so that to be handled by stateoftheart sat tools as with most previous approaches we begin our investigation from the satisfiability in km we propose an efficient encoding and we test it on an extensive set of benchmarks comparing the approach with the main stateoftheart tools available although the encoding is necessarily worstcase exponential from our experiments we notice that in practice this approach can handle most or all the problems which are at the reach of  the other approaches with performances which are comparable with or even better than those of the current stateoftheart tools



r  daly and q  shen 2009 learning bayesian network equivalence classes with ant colony optimization volume 35 pages 391447

bayesian networks are a useful tool in the representation of uncertain knowledge this paper proposes a new algorithm called acoe to learn the structure of a bayesian network it does this by conducting a search through the space of equivalence classes of bayesian networks using ant colony optimization aco to this end two novel extensions of traditional aco techniques are proposed and implemented firstly multiple types of moves are allowed secondly moves can be given in terms of indices that are not based on construction graph nodes the results of testing show that acoe performs better than a greedy search and other stateoftheart and metaheuristic algorithms whilst searching in the space of equivalence classes



f  bromberg d  margaritis and v  honavar 2009 efficient markov network structure discovery using independence tests volume 35 pages 449484

  we present two algorithms for learning the structure of a markov network from data  gsmn and gsimn  both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests  until very recently algorithms for structure learning were based on maximum likelihood estimation which has been proved to be nphard for markov networks due to the difficulty of estimating the parameters of the network needed for the computation of the data likelihood  the independencebased approach does not require the computation of the likelihood and thus both gsmn and gsimn can compute the structure efficiently as shown in our experiments  gsmn is an adaptation of the growshrink algorithm of margaritis and thrun for learning the structure of bayesian networks  gsimn extends gsmn by additionally exploiting pearls wellknown properties of the conditional independence relation to infer novel independences from known ones thus avoiding the performance of statistical tests to estimate them  to accomplish this efficiently gsimn uses the triangle theorem also introduced in this work which is a simplified version of the set of markov axioms  experimental comparisons on artificial and realworld data sets show gsimn can yield significant savings with respect to gsmn while generating a markov network with comparable or in some cases improved quality  we also compare gsimn to a forwardchaining implementation called gsimnfch that produces all possible conditional independences resulting from repeatedly applying  pearls theorems on the known conditional independence tests   the results of this comparison show that gsimn by the sole use of the triangle theorem is nearly optimal in terms of the set of independences tests that it infers




p  faliszewski e  hemaspaandra and l  a hemaspaandra 2009 how hard is bribery in elections volume 35 pages 485532

we study the complexity of influencing elections through bribery how computationally complex is it for an external actor to determine whether by paying certain voters to change their preferences a specified candidate can be made the elections winner  we study this problem for election systems as varied as scoring protocols and dodgson voting and in a variety of settings regarding homogeneousvsnonhomogeneous electorate bribability boundedsizevsarbitrarysized candidate sets weightedvsunweighted voters and succinctvsnonsuccinct input specification  we obtain both polynomialtime bribery algorithms and proofs of the intractability of bribery and indeed our results show that the complexity of bribery is extremely sensitive to the setting  for example we find settings in which bribery is npcomplete but manipulation by voters is in p and we find settings in which bribing weighted voters is npcomplete but bribing voters with individual bribe thresholds is in p  for the broad class of elections including plurality borda kapproval and veto known as scoring protocols we prove a dichotomy result for bribery of weighted voters we find a simpletoevaluate condition that classifies every case as either npcomplete or in p



j  e gallardo c  cotta and a  j fern225ndez 2009 solving weighted constraint satisfaction problems with memeticexact hybrid algorithms volume 35 pages 533555

a weighted constraint satisfaction problem wcsp is a constraint satisfaction problem in which preferences among solutions can be expressed bucket elimination is a complete technique commonly used to solve this kind of constraint satisfaction problem when the memory required to apply bucket elimination is too high a heuristic method based on it denominated minibuckets can be used to calculate bounds for the optimal solution nevertheless the curse of dimensionality makes these techniques impractical on large scale problems in response to this situation we present a memetic algorithm for wcsps in which bucket elimination is used as a mechanism for recombining solutions providing the best possible child from the parental set subsequently a multilevel model in which this exactmetaheuristic hybrid is further hybridized with branchandbound techniques and minibuckets is studied as a case study we have applied these algorithms to the resolution of the maximum density still life problem a hard constraint optimization problem based on conways game of life the resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches moreover it is shown that this proposal provides new best known solutions for very large instances



a  krause and c  guestrin 2009 optimal value of information in graphical models volume 35 pages 557591




honorable mention for the 2012 ijcaijair best paper prize

many realworld decision making tasks require us to choose among several expensive observations in a sensor network for example it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty in medical decision making tasks one needs to select which tests to administer before deciding on the most effective treatment it has been general practice to use heuristicguided procedures for selecting observations in this paper we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models for example our algorithms allow to optimally label hidden variables in hidden markov models hmms we provide results for both selecting the optimal subset of observations and for obtaining an optimal conditional observation plan
in addition we consider several extensions such as using our algorithms for scheduling observation selection for multiple sensors we demonstrate the effectiveness of our approach on several realworld datasets including a prototype sensor network deployment for energy conservation in buildings



m  zytnicki c  gaspin s  de givry and t  schiex 2009 bounds arc consistency for weighted csps volume 35 pages 593621

  the weighted constraint satisfaction problem wcsp framework allows representing and solving problems involving both hard constraints and cost functions it has been applied to various problems including resource allocation bioinformatics scheduling etc to solve such problems solvers usually rely on branchandbound algorithms equipped with local consistency filtering mostly soft arc consistency  however these techniques are not well suited to solve problems with very large domains motivated by the resolution of an rna gene localization problem inside large genomic sequences and in the spirit of bounds consistency for large domains in crisp csps we introduce soft bounds arc consistency a new weighted local consistency specifically designed for wcsp with very large domains compared to  soft arc consistency bac provides significantly improved time and space asymptotic complexity in this paper we show how the semantics of cost functions can be exploited to further improve the time complexity of bac we also compare both in theory and in practice the efficiency of bac on a wcsp  with bounds consistency enforced on a crisp csp using cost variables on two different real problems modeled as wcsp including our rna gene localization problem we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables




h  palacios and h  geffner 2009 compiling uncertainty away in conformant planning problems with bounded width volume 35 pages 623675




2012 ijcaijair best paper prize

conformant planning is the problem of finding a sequence of actions for achieving a goal in the presence of uncertainty in the initial state or action effects  the problem has been approached as a pathfinding problem in belief space where good belief representations and heuristics are critical for scaling up  in this work a different formulation is introduced for conformant problems with deterministic actions where they are automatically converted into classical ones and solved by an offtheshelf classical planner  the translation maps literals l and sets of assumptions t about the initial situation into new literals klt that represent that l must be true if t is initially true  we lay out a general translation scheme that is sound and establish the conditions under which the translation is also complete  we show that the complexity of the complete translation is exponential in a parameter of the problem called the conformant width which for most benchmarks is bounded the planner based on this translation exhibits good performance in comparison with existing planners and is the basis for t0 the best performing planner in the conformant track of the 2006 international planning competition



k  su a  sattar g  lv and y  zhang 2009 variable forgetting in reasoning about knowledge volume 35 pages 677716

in this paper we investigate knowledge reasoning within a simple framework called knowledge structure we use variable forgetting as a basic operation for one agent to reason about its own or other agents knowledge in our framework two notions namely agents observable variables and the weakest sufficient condition play important roles in knowledge reasoning given a background knowledge base and a set of observable variables for each agent we show that the notion of an agent knowing a formula can be defined as a weakest sufficient condition of the formula under background knowledge base moreover we show how to capture the notion of common knowledge by using a generalized notion of weakest sufficient condition also we show that public announcement operator can be conveniently dealt with via our notion of knowledge structure further we explore the computational complexity of the problem whether an epistemic formula is realized in a knowledge structure in the general case this problem is pspacehard however for some interesting subcases it can be reduced to conp finally we discuss possible applications of our framework in some interesting domains such as the automated analysis of the wellknown muddy children puzzle and the verification of the revised needhamschroeder protocol we believe that there are many scenarios where the natural presentation of the available information about knowledge is under the form of a knowledge structure what makes it valuable compared with the corresponding multiagent s5 kripke structure is that it can be much more succinct



p  a bonatti c  lutz and f  wolter 2009 the complexity of circumscription in dls volume 35 pages 717773

as fragments of firstorder logic description logics dls do not provide nonmonotonic features such as defeasible inheritance and default rules since many applications would benefit from the availability of such features several families of nonmonotonic dls have been developed that are mostly based on default logic and autoepistemic logic in this paper we consider circumscription as an interesting alternative approach to nonmonotonic dls that in particular supports defeasible inheritance in a natural way we study dls extended with circumscription under different language restrictions and under different constraints on the sets of minimized fixed and varying predicates and pinpoint the exact computational complexity of reasoning for dls ranging from alc to alcio and alcqo  when the minimized and fixed predicates include only concept names but no role names then reasoning is complete for nexptimenp  it becomes complete for npnexptime when the number of minimized and fixed predicates is bounded by a constant  if roles can be minimized or fixed then complexity ranges from nexptimenp to undecidability



e  saquete j  luis vicedo p  mart237nezbarco r  mu241oz and h  llorens 2009 enhancing qa systems with complex temporal question processing capabilities volume 35 pages 775811

this paper presents a multilayered architecture that enhances the capabilities of current qa systems and allows different types of complex questions or queries to be processed the answers to these questions need to be gathered from factual information scattered throughout different documents specifically we designed a specialized layer to process the different types of temporal questions complex temporal questions are first decomposed into simple questions according to the temporal relations expressed in the original question in the same way the answers to the resulting simple questions are recomposed fulfilling the temporal restrictions of the original complex question a novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources with the final aim of obtaining a portable platform that is easily extensible to other languages in this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level the temporal layer was first performed for english then evaluated and compared with a a general purpose qa system fmeasure 6547 for qa plus english temporal layer vs 3801 for the general qa system and b a wellknown qa system much better results were obtained for temporal questions with the multilayered system this system was therefore extended to spanish and very good results were again obtained in the evaluation fmeasure 4036 for qa plus spanish temporal layer vs 2294 for the general qa system



t  janhunen e  oikarinen h  tompits and s  woltran 2009 modularity aspects of disjunctive stable models volume 35 pages 813857

practically all programming languages allow the programmer to split a program into several modules which brings along several advantages in software development in this paper we are interested in the area of answerset programming where fully declarative and nonmonotonic languages are applied in this context obtaining a modular structure for programs is by no means straightforward since the output of an entire program cannot in general be composed from the output of its components to better understand the effects of disjunctive information on modularity we restrict the scope of analysis to the case of disjunctive logic programs dlps subject to stablemodel semantics we define the notion of a dlpfunction where a welldefined inputoutput interface is provided and establish a novel module theorem which indicates the compositionality of stablemodel semantics for dlpfunctions the module theorem extends the wellknown splittingset theorem and enables the decomposition of dlpfunctions given their strongly connected components based on positive dependencies induced by rules in this setting it is also possible to split shared disjunctive rules among components using a generalized shifting technique the concept of modular equivalence is introduced for the mutual comparison of dlpfunctions using a generalization of a translationbased verification method
