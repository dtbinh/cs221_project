gert  de cooman jasper  de bock and m225rcio  alves diniz 2015 coherent predictive inference under exchangeability with imprecise probabilities volume 52 pages 195

coherent reasoning under uncertainty can be represented in a very general manner by coherent sets of desirable gambles in a context that does not allow for indecision this leads to an approach that is mathematically equivalent to working with coherent conditional probabilities if we do allow for indecision this leads to a more general foundation for coherent impreciseprobabilistic inference in this framework and for a given finite category set coherent predictive inference under exchangeability can be represented using bernstein coherent cones of multivariate polynomials on the simplex generated by this category set this is a powerful generalisation of de finettis representation theorem allowing for both imprecision and indecision
we define an inference system as a map that associates a bernstein coherent cone of polynomials with every finite category set many inference principles encountered in the literature can then be interpreted and represented mathematically as restrictions on such maps we discuss as particular examples two important inference principles representation insensitivitya strengthened version of walleys representation invarianceand specificity we show that there is an infinity of inference systems that satisfy these two principles amongst which we discuss in particular the skeptically cautious inference system the inference systems corresponding to a modified version of walley and bernards imprecise dirichlet multinomial models idmm the skeptical idmm inference systems and the haldane inference system we also prove that the latter produces the same posterior inferences as would be obtained using haldanes improper prior implying that there is an infinity of proper priors that produce the same coherent posterior inferences as haldanes improper one finally we impose an additional inference principle that allows us to characterise uniquely the immediate predictions for the idmm inference systems



carmel  domshlak and vitaly  mirkis 2015 deterministic oversubscription planning as heuristic search abstractions and reformulations volume 52 pages 97169

while in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible the objective in deterministic oversubscription planning osp is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost although numerous applications in various fields share the latter objective no substantial algorithmic advances have been made in deterministic osp tracing the key sources of progress in classical planning we identify a severe lack of effective domainindependent approximations for osp 
with our focus here on optimal planning our goal is to bridge this gap two classes of approximation techniques have been found especially useful in the context of optimal classical planning those based on statespace  abstractions and those based on logical landmarks for goal reachability the question we  study here is whether some similarinspirit yet possibly mathematically  different approximation techniques can be developed for osp in the context of abstractions we define the notion of additive abstractions for osp study the complexity of deriving effective abstractions from a rich space of hypotheses and reveal some  substantial empirically relevant islands of tractability in the context of  landmarks we show how standard goalreachability landmarks of certain classical planning tasks  can be compiled into the osp task of interest resulting in an equivalent osp task with a lower cost allowance and thus with a smaller search space  our empirical evaluation confirms the effectiveness of the proposed techniques and opens a wide gate for further developments in oversubscription planning  



yair  wiener and ran  elyaniv 2015 agnostic pointwisecompetitive selective classification volume 52 pages 171201

pointwisecompetitive classifier from class f is required to classify identically to the best classifier in hindsight from f for noisy agnostic settings we present a strategy for learning pointwisecompetitive classifiers from a finite training sample provided that the classifier can abstain from prediction at a certain region of its choice for some interesting hypothesis classes and families of distributions the measure of this rejected region is shown to be diminishing at a fast rate with high probability exact implementation of the proposed learning strategy is dependent on an erm oracle that can be hard to compute in the agnostic case we thus consider a heuristic approximation procedure that is based on svms and show empirically that this algorithm consistently outperforms a traditional rejection mechanism based on distance from decision boundary



ronald  de haan iyad  kanj and stefan  szeider 2015 on the subexponentialtime complexity of csp volume 52 pages 203234

not all npcomplete problems share the same practical hardness with respect to exact computation  whereas some npcomplete problems are amenable to efficient computational methods others are yet to show any such sign it becomes a major challenge to develop a theoretical framework that is more finegrained than the theory of npcompleteness and that can explain the distinction between the exact complexities of various npcomplete problems this distinction is highly relevant for constraint satisfaction problems under natural restrictions where various shades of hardness can be observed in practice




broes  de cat marc  denecker maurice  bruynooghe and peter  stuckey 2015 lazy model expansion interleaving grounding with search volume 52 pages 235286

finding satisfying assignments for the variables involved in a set of constraints can be cast as a bounded model generation problem search for bounded models of a theory in some logic the stateoftheart approach for bounded model generation for rich knowledge representation languages like asp and fo and a csp modeling language such as zinc is groundandsolve reduce the theory to a ground or propositional one and apply a search algorithm to the resulting theory
an important bottleneck is the blowup of the size of the theory caused by the grounding phase lazily grounding the theory during search is a way to overcome this bottleneck we present a theoretical framework and an implementation in the context of the fo knowledge representation language instead of grounding all parts of a theory justifications are derived for some parts of it given a partial assignment for the grounded part of the theory and valid justifications for the formulas of the nongrounded part the justifications provide a recipe to construct a complete assignment that satisfies the nongrounded part when a justification for a particular formula becomes invalid during search a new one is derived if that fails the formula is split in a part to be grounded and a part that can be justified experimental results illustrate the power and generality of this approach



paolo  liberatore 2015 revision by history volume 52 pages 287329

this article proposes a solution to the problem of obtaining plausibility information which is necessary to perform belief revision given a sequence of revisions together with their results derive a possible initial order that has generated them this is different from the usual assumption of starting from an allequal initial order and modifying it by a sequence of revisions four semantics for iterated revision are considered natural restrained lexicographic and reinforcement for each a necessary and sufficient condition to the existence of an order generating a given history of revisions and results is proved complexity is proved conp complete in all cases but one reinforcement revision with unbounded sequence length 




shan  xue alan  fern and daniel  sheldon 2015 scheduling conservation designs for maximum flexibility via network cascade optimization volume 52 pages 331360

one approach to conserving endangered species is to purchase and protect a set of land parcels in a way that maximizes the expected future population spread unfortunately an ideal set of parcels may have a cost that is beyond the immediate budget constraints and must thus be purchased incrementally this raises the challenge of deciding how to schedule the parcel purchases in a way that maximizes the flexibility of budget usage while keeping population spread loss in control in this paper we introduce a formulation of this scheduling problem that does not rely on knowing the future budgets of an organization in particular we consider scheduling purchases in a way that achieves a population spread no less than desired but delays purchases as long as possible such schedules offer conservation planners maximum flexibility and use available budgets in the most efficient way we develop the problem formally as a stochastic optimization problem over a network cascade model describing a commonly used model of population spread our solution approach is based on reducing the stochastic problem to a novel variant of the directed steiner tree problem which we call the setweighted directed steiner graph problem we show that this problem is computationally hard motivating the development of a primaldual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution we evaluate the approach on both real and synthetic conservation data with a standard population spread model the algorithm is shown to produce near optimal results and is much more scalable than more generic offtheshelf optimizers finally we evaluate a variant of the algorithm to explore the tradeoffs between budget savings and population growth



been  kim caleb  m chacha and julie  a shah 2015 inferring team task plans from human meetings a generative modeling approach with logicbased prior volume 52 pages 361398

we aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in timecritical domains such as military field operations and disaster response deployment plans for these operations are frequently negotiated onthefly by teams of human planners a human operator then translates the agreedupon plan into machine instructions for the robots we present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human teams planning conversation our hybrid approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans enabling us to overcome the challenge of performing inference over a large solution space with only a small amount of noisy data from the team planning session we validate the algorithm through human subject experimentations and show that it is able to infer a human teams final plan with 86 accuracy on average we also describe a robot demonstration in which two people plan and execute a firstresponse collaborative task with a pr2 robot to the best of our knowledge this is the first work to integrate a logical planning technique within a generative model to perform plan inference 



diederik  marijn roijers shimon  whiteson and frans  a oliehoek 2015 computing convex coverage sets for faster multiobjective coordination volume 52 pages 399443

in this article we propose new algorithms for multiobjective coordination graphs mocogs key to the efficiency of these algorithms is that they compute a convex coverage set ccs instead of a pareto coverage set pcs not only is a ccs a sufficient solution set for a large class of problems it also has important characteristics that facilitate more efficient solutions we propose two main algorithms for computing a ccs in mocogs convex multiobjective variable elimination cmove computes a ccs by performing a series of agent eliminations which can be seen as solving a series of local multiobjective subproblems variable elimination linear support vels iteratively identifies the single weight vector w that can lead to the maximal possible improvement on a partial ccs and calls variable elimination to solve a scalarized instance of the problem for w vels is faster than cmove for small and medium numbers of objectives and can compute an 949approximate ccs in a fraction of the runtime in addition we propose variants of these methods that employ andor tree search instead of variable elimination to achieve memory efficiency we analyze the runtime and space complexities of these methods prove their correctness and compare them empirically against a naive baseline and an existing pcs method both in terms of memoryusage and runtime  our results show that by focusing on the ccs these methods achieve much better scalability in the number of agents than the current state of the art



mariecatherine  de marneffe marta  recasens and christopher  potts 2015 modeling the lifespan of discourse entities with application to coreference resolution volume 52 pages 445475

a discourse typically involves numerous entities but few are mentioned more than once distinguishing those that die out after just one mention singleton from those that lead longer lives coreferent would dramatically simplify the hypothesis space for coreference resolution models leading to increased performance to realize these gains we build a classifier for predicting the singletoncoreferent distinction the models feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans especially negation modality and attitude predication with existing results about the benefits of surface partofspeech and ngrambased features for coreference resolution the model is effective in its own right and the feature representations help to identify the anchor phrases in bridging anaphora as well furthermore incorporating the model into two very different stateoftheart coreference resolution systems one rulebased and the other learningbased yields significant performance improvements



athirai  a irissappane and jie   zhang 2015 a casebased reasoning framework to choose trust models for different emarketplace environments volume 52 pages 477505

the performance of trust models highly depend on the characteristics of the environments where they are applied thus it becomes challenging to choose a suitable trust model for a given emarketplace environment especially when ground truth about the agent buyer and seller behavior is unknown called unknown environment we propose a casebased reasoning framework to choose suitable trust models for unknown environments based on the intuition that if a trust model performs well in one environment it will do so in another similar environment firstly we build a case base with a number of simulated environments with known ground truth along with the trust models most suitable for each of them given an unknown environment casebased retrieval algorithms retrieve the most similar cases and the trust model of the most similar cases is chosen as the most suitable model for the unknown environment evaluation results confirm the effectiveness of our framework in choosing suitable trust models for different emarketplace environments



piotr  faliszewski edith  hemaspaandra and lane  a hemaspaandra 2015 weighted electoral control volume 52 pages 507542




aamas 2013 best paper award finalist

although manipulation and bribery have been extensively studied under weighted voting there has been almost no work done on election control under weighted voting this is unfortunate since weighted voting appears in many important natural settings in this paper we study the complexity of controlling the outcome of weighted elections through adding and deleting voters we obtain polynomialtime algorithms npcompleteness results and for many npcomplete cases approximation algorithms in particular for scoring rules we completely characterize the complexity of weighted voter control our work shows that for quite a few important cases either polynomialtime exact algorithms or polynomialtime approximation algorithms exist



minh  daotran thomas  eiter michael  fink and thomas  krennwallner 2015 distributed evaluation of nonmonotonic multicontext systems volume 52 pages 543600

multicontext systems mcss are a formalism for systems consisting of knowledge bases possibly heterogeneous and nonmonotonic that are interlinked via bridge rules where the global system semantics emerges from the local semantics of the knowledge bases also called contexts in an equilibrium while mcss and related formalisms are inherently targeted for distributed set tings no truly distributed algorithms for their evaluation were available we address this short coming and present a suite of such algorithms which includes a basic algorithm dmcs an ad vanced version dmcsopt that exploits topologybased optimizations and a streaming algorithm dmcsstreaming that computes equilibria in packages of bounded size the algorithms be have quite differently in several respects as experienced in thorough experimental evaluation of a system prototype from the experimental results we derive a guideline for choosing the appropriate algorithm and running mode in particular situations determined by the parameter settings



haonan  yu n  siddharth andrei  barbu and jeffrey  mark siskind 2015 a compositional framework for grounding language inference generation and acquisition in video volume 52 pages 601713




acl 2013 best paper award

we present an approach to simultaneously reasoning about a video clip and an entire naturallanguage sentence the compositional nature of language is exploited to construct models which represent the meanings of entire sentences composed out of the meanings of the words in those sentences mediated by a grammar that encodes the predicateargument relations we demonstrate that these models faithfully represent the meanings of sentences and are sensitive to how the roles played by participants nouns their characteristics adjectives the actions performed verbs the manner of such actions adverbs and changing spatial relations between participants prepositions affect the meaning of a sentence and how it is grounded in video we exploit this methodology in three ways in the first a video clip along with a sentence are taken as input and the participants in the event described by the sentence are highlighted even when the clip depicts multiple similar simultaneous events in the second a video clip is taken as input without a sentence and a sentence is generated that describes an event in that clip in the third a corpus of video clips is paired with sentences which describe some of the events in those clips and the meanings of the words in those sentences are learned we learn these meanings without needing to specify which attribute of the video clips each word in a given sentence refers to the learned meaning representations are shown to be intelligible to humans

