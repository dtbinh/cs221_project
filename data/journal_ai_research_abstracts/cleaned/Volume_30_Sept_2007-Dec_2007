m  j carman and c  a knoblock 2007 learning semantic definitions of online information sources volume 30 pages 150

the internet contains a very large number of information sources providing many types of data from weather forecasts to travel deals and financial information these sources can be accessed via webforms web services rss feeds and so on in order to make automated use of these sources we need to model them semantically but writing semantic descriptions for web services is both tedious and error prone in this paper we investigate the problem of automatically generating such models we introduce a framework for learning datalog definitions of web sources in order to learn these definitions our system actively invokes the sources and compares the data they produce with that of known sources of information it then performs an inductive logic search through the space of plausible source definitions in order to learn the best possible semantic model for each new source in this paper we perform an empirical evaluation of the system using realworld web sources the evaluation demonstrates the effectiveness of the approach showing that we can automatically learn complex models for real sources in reasonable time we also compare our system with a complex schema matching system showing that our approach can handle the kinds of problems tackled by the latter



v  bulitko n  sturtevant j  lu and t  yau 2007 graph abstraction in realtime heuristic search volume 30 pages 51100

realtime heuristic search methods are used by situated agents in applications that require the amount of planning per move to be independent of the problem size such agents plan only a few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time we extend a wide class of realtime search algorithms with automaticallybuilt state abstraction and prove completeness and convergence of the resulting family of algorithms we then analyze the impact of abstraction in an extensive empirical study in realtime pathfinding abstraction is found to improve efficiency by providing better trading offs between planning time learning speed and other negatively correlated performance measures




m  pistore and m  y vardi 2007 the planning spectrum  one two three infinity volume 30 pages 101132

linear temporal logic ltl is widely used for defining conditions on the execution paths of dynamic systems  in the case of dynamic systems that allow for nondeterministic evolutions one has to specify along with an ltl formula f which are the paths that are required to satisfy the formula  two extreme cases are the universal interpretation af which requires that the formula be satisfied for all execution paths and the existential interpretation ef which requires that the formula be satisfied for some execution path




j  l bredin d  c parkes and q  duong 2007 chain a dynamic double auction framework for matching patient agents  volume 30 pages 133179

in this paper we present and evaluate a general framework for the design of truthful auctions for matching agents in a dynamic twosided market a single commodity such as a resource or a task is bought and sold by multiple buyers and sellers that arrive and depart over time our algorithm chain provides the first framework that allows a truthful dynamic double auction da to be constructed from a truthful singleperiod  ie static doubleauction rule the pricing and matching method of the chain construction is unique amongst dynamicauction rules that adopt the same building block  we examine experimentally the allocative efficiency of chain when instantiated on various singleperiod rules including the canonical mcafee doubleauction rule for a baseline we also consider nontruthful double auctions populated with zerointelligence plusstyle learning agents chainbased auctions perform well in comparison with other schemes especially as arrival intensity falls and agent valuations become more volatile 



s  p ponzetto and m  strube 2007 knowledge derived from wikipedia for computing semantic relatedness volume 30 pages 181212




honorable mention for the 2010 ijcaijair best paper prize

wikipedia provides a semantic network for computing semantic relatedness in a more structured fashion than a search engine and with more coverage than wordnet we present experiments on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets existing relatedness measures perform better using wikipedia than a baseline given by google counts and we show that wikipedia outperforms wordnet on some datasets we also address the question whether and how wikipedia can be integrated into nlp applications as a knowledge base including wikipedia improves the performance of a machine learning based coreference resolution system indicating that it represents a valuable resource for nlp applications finally we show that our method can be easily used for languages other than english by computing semantic relatedness for a german dataset



a  felner r  e korf r  meshulam and r  c holte 2007 compressed pattern databases volume 30 pages 213247

a pattern database pdb is a heuristic function implemented as a lookup table that stores the lengths of optimal solutions for subproblem instances standard pdbs have a distinct entry in the table for each subproblem instance in this paper we investigate compressing pdbs by merging several entries into one thereby allowing the use of pdbs that exceed available memory in their uncompressed form we introduce a number of methods for determining which entries to merge and discuss their relative merits these vary from domainindependent approaches that allow any set of entries in the pdb to be merged to more intelligent methods that take into account the structure of the problem the choice of the best compression method is based on domaindependent attributes we present experimental results on a number of combinatorial problems including the fourpeg towers of hanoi problem the slidingtile puzzles and the topspin puzzle for the towers of hanoi we show that the search time can be reduced by up to three orders of magnitude by using compressed pdbs compared to uncompressed pdbs of the same size more modest improvements were observed for the other domains



a  mccallum x  wang and a  corradaemmanuel 2007 topic and role discovery in social networks with experiments on enron and academic email volume 30 pages 249272

previous work in social network analysis sna has modeled the existence of links from one entity to another but not the attributes such as language content or topics on those links we present the authorrecipienttopic art model for social network analysis which learns topic distributions based on the directionsensitive messages sent between entities the model builds on latent dirichlet allocation lda and the authortopic at model adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipientsteering the discovery of topics according to the relationships between people we give results on both the enron email corpus and a researchers email archive providing evidence not only that clearly relevant topics are discovered but that the art model better predicts peoples roles and gives lower perplexity on previously unseen messages we also present the roleauthorrecipienttopic rart model an extension to art that explicitly represents peoples roles



g  stoilos g  stamou j  z pan v  tzouvaras and i  horrocks 2007 reasoning with very expressive fuzzy description logics volume 30 pages 273320

it is widely recognized today that the management of imprecision and vagueness will yield more intelligent and realistic knowledgebased applications description logics dls are a family of knowledge representation languages that have gained considerable attention the last decade mainly due to their decidability and the existence of empirically high performance of reasoning algorithms in this paper we extend the well known fuzzy alc dl to the fuzzy shin dl which extends the fuzzy alc dl with transitive role axioms s inverse roles i role hierarchies h and number restrictions n we illustrate why transitive role axioms are difficult to handle in the presence of fuzzy interpretations and how to handle them properly then we extend these results by adding role hierarchies and finally number restrictions the main contributions of the paper are the decidability proof of the fuzzy dl languages fuzzysi and fuzzyshin as well as decision procedures for the knowledge base satisfiability problem of the fuzzysi and fuzzyshin



c  m li f  manya and j  planes 2007 new inference rules for maxsat volume 30 pages 321359

exact maxsat solvers compared with sat solvers apply little inference at each node of the proof tree commonly used sat inference rules like unit propagation produce a simplified formula that preserves satisfiability but unfortunately solving the maxsat problem for the simplified formula is not equivalent to solving it for the original formula in this paper we define a number of original inference rules that besides being applied efficiently transform maxsat instances into equivalent maxsat instances which are easier to solve the soundness of the rules that can be seen as refinements of unit resolution adapted to maxsat are proved in a novel and simple way via an integer programming transformation with the aim of finding out how powerful the inference rules are in practice we have developed a new maxsat solver called maxsatz which incorporates those rules and performed  an experimental investigation the results provide empirical evidence that maxsatz is very competitive at least on random max2sat random max3sat maxcut and graph 3coloring instances as well as on the benchmarks from the maxsat evaluation 2006



j  bell 2007 natural events volume 30 pages 361412

this paper develops an inductive theory of predictive common sense reasoning the theory  provides the basis for an integrated solution to the three traditional problems of reasoning about change  the frame qualification and ramification problems the theory is also capable of representing nondeterministic events and it provides a means for stating defeasible preferences over the outcomes of conflicting simultaneous events 



m  a walker a  stent f  mairesse and r  prasad 2007 individual and domain adaptation in sentence planning for dialogue volume 30 pages 413456

one of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module this challenge arises from the need for the generator to adapt to many features of the dialogue domain user population and dialogue context  a promising approach is trainable generation which uses generalpurpose linguistic knowledge that is automatically adapted to the features of interest such as the application domain individual user or user group  in this paper we present and evaluate a trainable sentence planner for providing restaurant information in the match dialogue system  we show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a templatebased generator tuned to this domain  we also show that our method easily supports adapting the sentence planner to individuals and that the individualized sentence planners generally perform better than models trained and tested on a population of individuals previous work has documented and utilized individual preferences for content selection but to our knowledge these results provide the first demonstration of individual preferences for sentence planning operations affecting the content order discourse structure and sentence structure of system responses finally we evaluate the contribution of different feature sets and show that in our application ngram features often do as well as features based on higherlevel linguistic representations



f  mairesse m  a walker m  r mehl and r  k moore 2007 using linguistic cues for the automatic recognition of personality in conversation and text volume 30 pages 457500

it is well known that utterances convey a great deal of information about the speaker in addition to their semantic content  one such type of information consists of cues to the speakers personality traits the most fundamental dimension of variation between humans  recent work explores the automatic detection of other types of pragmatic variation in text and conversation such as emotion deception speaker charisma dominance point of view subjectivity opinion and sentiment personality affects these other aspects of linguistic production and thus personality recognition may be useful for these tasks in addition to many other potential applications  however to date there is little work on the automatic recognition of personality traits  this article reports experimental results for recognition of all big five personality traits in both conversation and text utilising both self and observer ratings of personality  while other work reports classification results we experiment with classification regression and ranking models for each model we analyse the effect of different feature sets on accuracy results show that for some traits any type of statistical model performs significantly better than the baseline but ranking models perform best overall we also present an experiment suggesting that ranking models are more accurate than multiclass classifiers for modelling personality in addition recognition models trained on observed personality perform better than models trained using selfreports and the optimal feature set depends on the personality trait a qualitative analysis of the learned models confirms previous findings linking language and personality while revealing many new linguistic markers



s  greco i  trubitsyna and e  zumpano 2007 on the semantics of logic programs with preferences volume 30 pages 501523

this work is a contribution to prioritized reasoning in logic programming in the presence of preference relations involving atoms the  technique providing a new interpretation for prioritized logic programs is inspired by the semantics of prioritized logic programming and enriched with the use of structural information of preference of answer set optimization programming specifically the analysis  of the logic program is carried out together with the analysis of preferences in order to determine the choice order and the sets of comparable models the new semantics is compared with other approaches known in the literature and complexity analysis is also performed showing that with respect to other similar approaches previously proposed the complexity of computing preferred stable models does not increase



s  i hill and a  doucet 2007 a framework for kernelbased multicategory classification volume 30 pages 525564

a geometric framework for understanding multicategory classification is introduced through which many existing alltogether algorithms can be understood  the structure enables parsimonious optimisation through a direct extension of the binary methodology  the focus is on support vector classification with parallels drawn to related methods
it is also described how this architecture provides insights regarding how to further improve on the speed of existing multicategory classification algorithms  an initial example of how this might be achieved is developed in the formulation of a straightforward multicategory sequential minimal optimisation algorithm  proofofconcept experimental results have shown that this combined with the mapping of pairwise results is comparable with benchmark optimisation speeds



c  domshlak and j  hoffmann 2007 probabilistic planning via heuristic forward search and weighted model counting volume 30 pages 565620

we present a new algorithm for probabilistic planning with no observability  our algorithm called  probabilisticff extends the heuristic forwardsearch machinery of conformantff to problems with probabilistic uncertainty about both the initial state and action effects specifically  probabilisticff combines conformantffs techniques with a powerful machinery for weighted model counting in weighted cnfs serving to elegantly define both the search space and the heuristic function our evaluation of  probabilisticff shows its fine scalability in a range of probabilistic domains constituting a several orders of magnitude improvement over previous results in this area we use a problematic case to point out the main open issue to be addressed by further research



i  bhattacharya and l  getoor 2007 querytime entity resolution volume 30 pages 621657

entity resolution is the problem of reconciling database references corresponding to the same realworld entities given the abundance of publicly available databases that have unresolved entities we motivate the problem of querytime entity resolution quick and accurate resolution for answering queries over such unclean databases at querytime  since collective entity resolution approaches  where related references are resolved jointly  have been shown to be more accurate than independent attributebased resolution for offline entity resolution we focus on developing new algorithms for collective resolution for answering entity resolution queries at querytime  for this purpose we first formally show that for collective resolution precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered unfolding this progression leads naturally to a two stage expand and resolve query processing strategy in this strategy we first extract the related records for a query using two novel expansion operators and then resolve the extracted records collectively we then show how the same strategy can be adapted for querytime entity resolution by identifying and resolving only those database references that are the most helpful for processing the query we validate our approach on two large realworld publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing we then show how the same queries can be answered in realtime using our adaptive approach while preserving the gains of collective resolution in addition to experiments on real datasets we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective entity resolution over a wide range of structural characteristics in the data



i  szita and a  lorincz 2007 learning to play using lowcomplexity rulebased policies illustrations through ms pacman volume 30 pages 659684

in this article we propose a method that can deal with certain combinatorial reinforcement learning tasks we demonstrate the approach in the popular ms pacman game we define a set of highlevel observation and action modules from which rulebased policies are constructed automatically in these policies actions are temporally extended and may work concurrently the policy of the agent is encoded by a compact decision list the components of the list are selected from a large pool of rules  which can be either handcrafted or generated automatically a suitable selection of rules is learnt  by the crossentropy method a recent global optimization algorithm that fits our framework smoothly  crossentropyoptimized policies perform better than our handcrafted policy and reach the score of average human players we argue that learning is successful mainly because i policies may apply concurrent actions and thus the policy space is sufficiently rich ii the search is biased towards lowcomplexity policies and therefore  solutions with a compact description can be found quickly if they exist

