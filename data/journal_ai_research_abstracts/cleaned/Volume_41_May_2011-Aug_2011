b  cseke and t  heskes 2011 properties of bethe free energies and message passing in gaussian models volume 41 pages 124

we address the problem of computing approximate marginals in gaussian probabilistic models by using mean field and fractional bethe approximations we define the gaussian fractional bethe free energy in terms of the moment parameters of the approximate marginals derive a lower and an upper bound on the fractional bethe free energy and establish a necessary condition for the lower bound to be bounded from below it turns out that the condition is identical to the pairwise normalizability condition which is known to be a sufficient condition for the convergence of the message passing algorithm we show that stable fixed points of the gaussian message passing algorithm are local minima of the gaussian bethe free energy by a counterexample we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm



l  xia and v  conitzer 2011 determining possible and necessary winners given partial orders volume 41 pages 2567

usually a voting rule requires agents to give their preferences as linear orders however in some cases it is impractical for an agent to give a linear order over all the alternatives it has been suggested to let agents submit partial orders instead then given a voting rule a profile of partial orders and an alternative candidate c two important questions arise first is it still possible for c to win and second is c guaranteed to win these are the possible winner and necessary winner problems respectively each of these two problems is further divided into two subproblems determining whether c is a unique winner that is c is the only winner or determining whether c is a cowinner that is c is in the set of winners 
we consider the setting where the number of alternatives is unbounded and the votes are unweighted we completely characterize the complexity of possiblenecessary winner problems for the following common voting rules a class of positional scoring rules including borda copeland maximin bucklin ranked pairs voting trees and plurality with runoff



m  bilgic and l  getoor 2011 value of information lattice exploiting probabilistic independence for effective feature subset acquisition volume 41 pages 6995

we address the costsensitive feature acquisition problem where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features because acquiring the features is costly as well the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized we describe the value of information lattice voila an optimal and efficient feature subset acquisition framework unlike the common practice which is to acquire features greedily voila can reason with subsets of features voila efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process through empirical evaluation on five medical datasets we show that the greedy strategy is often reluctant to acquire features as it cannot forecast the benefit of acquiring multiple features in combination



e  hebrard d  marx b  osullivan and i  razgon 2011 soft constraints of difference and equality volume 41 pages 97130

in many combinatorial problems one may need to model the diversity or similarity of assignments in a solution for example one may wish to maximise or minimise the number of distinct values in a solution to formulate problems of this type we can use soft variants of the well known alldifferent and allequal constraints  we present a taxonomy of six soft global constraints generated by combining the two latter ones and the two standard cost functions which are either maximised or minimised we characterise the complexity of achieving arc and bounds consistency on these constraints resolving those cases for which nphardness was neither proven nor disproven in particular we explore in depth the constraint ensuring that at least k pairs of variables have a common value we show that achieving arc consistency is nphard however achieving bounds consistency can be done in polynomial time through dynamic programming moreover we show that the maximum number of pairs of equal variables can be approximated by a factor 12 with a linear time greedy algorithm finally we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains interestingly this taxonomy shows that enforcing equality is harder than enforcing difference



s  p gujar and y  narahari 2011 redistribution mechanisms for assignment of heterogeneous objects volume 41 pages 131154

there are p heterogeneous objects to be assigned to n competing agents n  p each with unit demand it is required to design a groves mechanism for this assignment problem satisfying weak budget balance individual rationality and minimizing the budget imbalance this calls for designing an appropriate rebate function when the objects are identical this problem has been solved which we refer as wco mechanism we measure the performance of such mechanisms by the redistribution index we first prove an impossibility theorem which rules out linear rebate functions with nonzero redistribution index in heterogeneous object assignment motivated by this theorem we explore two approaches to get around this impossibility in the first approach we show that linear rebate functions with nonzero redistribution index are possible when the valuations for the objects have a certain type of relationship and we design a mechanism with linear rebate function that is worst case optimal in the second approach we show that rebate functions with nonzero efficiency are possible if linearity is relaxed we extend the rebate functions of the wco mechanism to heterogeneous objects assignment and conjecture them to be worst case optimal



j  hoffmann 2011 analyzing search topology without running any search on the   connection between causal graphs and h volume 41 pages 155229

the ignoring delete lists relaxation is of paramount importance for both satisficing and optimal planning in earlier work it was observed that the optimal relaxation heuristic h has amazing qualities in many classical planning benchmarks in particular pertaining to the complete absence of local minima the proofs of this are handmade raising the question whether such proofs can be lead automatically by domain analysis techniques in contrast to earlier disappointing results  the analysis method has exponential runtime and succeeds only in two extremely simple benchmark domains  we herein answer this question in the affirmative we establish connections between causal graph structure and h topology this results in loworder polynomial time analysis methods implemented in a tool we call torchlight of the 12 domains where the absence of local minima has been proved torchlight gives strong success guarantees in 8 domains empirically its analysis exhibits strong performance in a further 2 of these domains plus in 4 more domains where local minima may exist but are rare in this way torchlight can distinguish easy domains from hard ones by summarizing structural reasons for analysis failure torchlight also provides diagnostic output indicating domain aspects that may cause local minima



s  joshi and r  khardon 2011 probabilistic relational planning  with first order decision diagrams volume 41 pages 231266

dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations in particular algebraic decision diagrams to capture domain dynamics and value functions  work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes  recent work introduced a first order variant of decision diagrams fodd and developed a value iteration algorithm for this representation this paper develops several improvements to the fodd algorithm that make the approach practical these include new reduction operators that decrease the size of the representation several speedup techniques and techniques for value approximation  incorporating these the paper presents a planning system foddplanner for solving relational stochastic planning problems  the system is evaluated on several domains including problems from the recent international planning competition and shows competitive performance with top ranking systems this is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning




a   khudyak kozorovitsky and o  kurland 2011 from identical to similar fusing retrieved lists based on interdocument similarities volume 41 pages 267296

methods for fusing document lists that were retrieved in response to a query often utilize the retrieval scores andor ranks of documents in the lists we present a novel fusion approach that is based on using in addition information induced from interdocument similarities specifically our methods let similar documents from different lists provide relevancestatus support to each other we use a graphbased method to model relevancestatus propagation between documents the propagation is governed by interdocumentsimilarities and by retrieval scores of documents in the lists empirical evaluation demonstrates the effectiveness of our methods in fusing trec runs the performance of our most effective methods transcends that of effective fusion methods that utilize only retrieval scores or ranks



d  korzhyk z  yin c  kiekintveld v  conitzer and m  tambe 2011 stackelberg vs nash in security games an extended investigation of interchangeability equivalence and uniqueness volume 41 pages 297327

there has been significant recent interest in gametheoretic approaches to security with much of the recent research focused on utilizing the leaderfollower stackelberg game model among the major applications are the armor program deployed at lax airport and the iris program in use by the us federal air marshals fams the foundational assumption for using stackelberg games is that security forces leaders acting first commit to a randomized strategy while their adversaries followers choose their best response after surveillance of this randomized strategy  yet in many situations a leader may face uncertainty about the followers surveillance capability previous work fails to address how a leader should compute her strategy given such uncertainty
we provide five contributions in the context of a general class of security games first we show that the nash equilibria in security games are interchangeable thus alleviating the equilibrium selection problem second under a natural restriction on security games any stackelberg strategy is also a nash equilibrium strategy and furthermore the solution is unique in a class of security games of which armor is a key exemplar third when faced with a follower that can attack multiple targets many of these properties no longer hold fourth we show experimentally that in most but not all games where the restriction does not hold the stackelberg strategy is still a nash equilibrium strategy but this is no longer true when the attacker can attack multiple targets finally as a possible direction for future research we propose an extensiveform game model that makes the defenders uncertainty about the attackers ability to observe explicit



s  a siddiqi and j  huang 2011 sequential diagnosis by abstraction volume 41 pages 329365

when a system behaves abnormally sequential diagnosis takes a sequence of measurements of the system until the faults causing the abnormality are identified and the goal is to reduce the diagnostic cost defined here as the number of measurements to propose measurement points previous work employs a heuristic based on reducing the entropy over a computed set of diagnoses this approach generally has good performance in terms of diagnostic cost but can fail to diagnose large systems when the set of diagnoses is too large focusing on a smaller set of probable diagnoses scales the approach but generally leads to increased average diagnostic costs in this paper we propose a new diagnostic framework employing four new techniques which scales to much larger systems with good performance in terms of diagnostic cost
first we propose a new heuristic for measurement point selection that can be computed efficiently without requiring the set of diagnoses once the system is modeled as a bayesian network and compiled into a logical form known as ddnnf second we extend hierarchical diagnosis a technique based on system abstraction from our previous work to handle probabilities so that it can be applied to sequential diagnosis to allow larger systems to be diagnosed third for the largest systems where even hierarchical diagnosis fails we propose a novel method that converts the system into one that has a smaller abstraction and whose diagnoses form a superset of those of the original system the new system can then be diagnosed and the result mapped back to the original system finally we propose a novel cost estimation function which can be used to choose an abstraction of the system that is more likely to provide optimal average cost experiments with iscas85 benchmark circuits indicate that our approach scales to all circuits in the suite except one that has a flat structure not susceptible to useful abstraction 



o  kurland and e  krikon 2011 the opposite of smoothing a language model approach to ranking queryspecific document clusters volume 41 pages 367395

exploiting information induced from queryspecific clustering of topretrieved documents has long been proposed as a means for improving precision at the very top ranks of the returned results we present a novel language model approach to ranking queryspecific clusters by the presumed percentage of relevant documents that they contain while most previous cluster ranking approaches focus on the cluster as a whole our model utilizes also information induced from documents associated with the cluster our model substantially outperforms previous approaches for identifying clusters containing a high relevantdocument percentage furthermore using the model to produce document ranking yields precisionattopranks performance that is consistently better than that of the initial ranking upon which clustering is performed the performance also favorably compares with that of a stateoftheart pseudofeedbackbased retrieval method




x  lu h  m schwartz and s  n givigi 2011 policy invariance under reward transformations for generalsum stochastic games volume 41 pages 397406

we extend the potentialbased shaping method from markov decision processes to multiplayer generalsum stochastic games we prove that the nash equilibria in a stochastic game remains unchanged after potentialbased shaping is applied to the environment the property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game



a  gy246rgy and l  kocsis 2011 efficient multistart strategies for local search algorithms volume 41 pages 407444

local search algorithms applied to optimization problems often suffer from getting trapped in a local optimum the common solution for this deficiency is to restart the algorithm when no progress is observed alternatively one can start multiple instances of a local search algorithm and allocate computational resources in particular processing time to the instances depending on their behavior hence a multistart strategy has to decide dynamically when to allocate additional resources to a particular instance and when to start new instances in this paper we propose multistart strategies motivated by works on multiarmed bandit problems and lipschitz optimization with an unknown constant the strategies continuously estimate the potential performance of each algorithm instance by supposing a convergence rate of the local search algorithm up to an unknown constant and in every phase allocate resources to those instances that could converge to the optimum for a particular range of the constant asymptotic bounds are given on the performance of the strategies in particular we prove that at most a quadratic increase in the number of times the target function is evaluated is needed to achieve the performance of a local search algorithm started from the attraction region of the optimum experiments are provided using spsa simultaneous perturbation stochastic approximation and kmeans as local search algorithms and the results indicate that the proposed strategies work well in practice and in all cases studied need only logarithmically more evaluations of the target function as opposed to the theoretically suggested quadratic increase



w  dvo345225k and s  woltran 2011 on the intertranslatability of argumentation semantics volume 41 pages 445475

translations between different nonmonotonic formalisms always have been an important topic in the field in particular to understand the knowledgerepresentation capabilities those formalisms offer  we provide such an investigation in terms of different semantics proposed for abstract argumentation frameworks a nonmonotonic yet simple formalism which received increasing interest within the last decade although the properties of these different semantics are nowadays well understood there are no explicit results about intertranslatability we provide such translations wrt different properties and also give a few novel complexity results which underlie some negative results



j  sturm c  stachniss and w  burgard 2011 a probabilistic framework for learning kinematic models of articulated objects volume 41 pages 477526

robots operating in domestic environments generally need to interact with articulated objects such as doors cabinets dishwashers or fridges in this work we present a novel probabilistic framework for modeling articulated objects as kinematic graphs vertices in this graph correspond to object parts while edges between them model their kinematic relationship in particular we present a set of parametric and nonparametric edge models and how they can robustly be estimated from noisy pose observations we furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks we finally present how the learned models can be generalized to new and previously unseen objects  in various experiments using real robots with different camera systems as well as in simulation we show that our approach is valid accurate and efficient further we demonstrate that our approach has a broad set of applications in particular for the emerging fields of mobile manipulation and service robotics




j  v graca k  ganchev l  coheur f  pereira and b  taskar 2011 controlling complexity in partofspeech induction volume 41 pages 527551

we consider the problem of fully unsupervised learning of grammatical partofspeech categories from unlabeled text the standard maximumlikelihood hidden markov model for this task performs poorly because of its weak inductive bias and large model capacity we address this problem by refining the model and modifying the learning objective to control its capacity via para metric and nonparametric constraints our approach enforces wordcategory association sparsity adds morphological and orthographic features and eliminates hardtoestimate parameters for rare words we develop an efficient learning algorithm that is not much more computationally intensive than standard training we also provide an opensource implementation of the algorithm our experiments on five diverse languages bulgarian danish english portuguese spanish achieve significant improvements compared with previous methods for the same task
