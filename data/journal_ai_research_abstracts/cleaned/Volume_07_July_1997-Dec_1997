j  y halpern 1997 defining relative likelihood in partiallyordered preferential structures volume 7 pages 124

starting with a likelihood or preference order on worlds we    extend it to a likelihood ordering on sets of worlds in a natural way    and examine the resulting logic  lewis earlier considered such a    notion of relative likelihood in the context of studying    counterfactuals but he assumed a total preference order on worlds    complications arise when examining partial orders that are not present    for total orders  there are subtleties involving the exact approach    to lifting the order on worlds to an order on sets of worlds  in    addition the axiomatization of the logic of relative likelihood in    the case of partial orders gives insight into the connection between    relative likelihood and default reasoning



t  drakengren and p  jonsson 1997 eight maximal tractable subclasses of allens algebra with metric time volume 7 pages 2545

this paper combines two important directions of research in temporal resoning that of finding maximal tractable subclasses of allens interval algebra and that of reasoning with metric temporal information eight new maximal tractable subclasses of allens interval algebra are presented some of them subsuming previously reported tractable algebras the algebras allow for metric temporal constraints on interval starting or ending points using the recent framework of horn dlrs two of the algebras can express the notion of sequentiality between intervals being the first such algebras admitting both qualitative and metric time



d  l mammen and  t  hogg 1997 a new look at the easyhardeasy pattern of combinatorial search difficulty volume 7 pages 4766

the easyhardeasy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning we test the generality of this explanation by examining one of its predictions if the number of solutions is held fixed by the choice of problems then increased pruning should lead to a monotonic decrease in search cost instead we find the easyhardeasy pattern in median search cost even when the number of solutions is held constant for some search methods this generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost in these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems rather than changing numbers of solutions



c  g nevillmanning and  i  h witten 1997 identifying hierarchical structure in sequences a lineartime algorithm volume 7 pages 6782

sequitur is an algorithm that infers a hierarchical    structure from a sequence of discrete symbols by replacing repeated    phrases with a grammatical rule that generates the phrase and    continuing this process recursively the result is a hierarchical    representation of the original sequence which offers insights into    its lexical structure the algorithm is driven by two constraints that    reduce the size of the grammar and produce structure as a byproduct     sequitur breaks new ground by operating incrementally moreover the    methods simple structure permits a proof that it operates in space    and time that is linear in the size of the input our implementation    can process 50000 symbols per second and has been applied to an    extensive range of real world sequences



m  tambe 1997 towards flexible teamwork volume 7 pages 83124




2012 ifaamas award for influential papers in autonomous agents and multiagent systems

many ai researchers are today striving to build agent teams    for complex dynamic multiagent domains with intended applications    in arenas such as education training entertainment information    integration and collective robotics  unfortunately uncertainties in    these complex dynamic domains obstruct coherent teamwork  in    particular team members often encounter differing incomplete and    possibly inconsistent views of their environment  furthermore team    members can unexpectedly fail in fulfilling responsibilities or    discover unexpected opportunities  highly flexible coordination and    communication is key in addressing such uncertainties  simply fitting    individual agents with precomputed coordination plans will not do for    their inflexibility can cause severe failures in teamwork and their    domainspecificity hinders reusability           our central hypothesis is that the key to such flexibility and    reusability is providing agents with general models of teamwork    agents exploit such models to autonomously reason about coordination    and communication providing requisite flexibility  furthermore the    models enable reuse across domains both saving implementation effort    and enforcing consistency  this article presents one general    implemented model of teamwork called steam  the basic building block    of teamwork in steam is joint intentions cohen  levesque 1991b    teamwork in steam is based on agents building up a partial    hierarchy of joint intentions this hierarchy is seen to parallel    grosz  krauss partial sharedplans 1996  furthermore in steam    team members monitor the teams and individual members performance    reorganizing the team as necessary  finally decisiontheoretic    communication selectivity in steam ensures reduction in communication    overheads of teamwork with appropriate sensitivity to the    environmental conditions  this article describes steams application    in three different complex domains and presents detailed empirical    results



l  leherte  j  glasgow  k  baxter  e  steeg and  s  fortier 1997 analysis of threedimensional protein images volume 7 pages 125159

a fundamental goal of research in molecular biology is to    understand protein structure protein crystallography is currently the    most successful method for determining the threedimensional 3d    conformation of a protein yet it remains labor intensive and relies    on an experts ability to derive and evaluate a protein scene model    in this paper the problem of protein structure determination is    formulated as an exercise in scene analysis  a computational    methodology is presented in which a 3d image of a protein is segmented    into a graph of critical points  bayesian and certainty factor    approaches are described and used to analyze critical point graphs and    identify meaningful substructures such as alphahelices and    betasheets  results of applying the methodologies to protein images    at low and medium resolution are reported  the research is related to    approaches to representation segmentation and classification in    vision as well as to topdown approaches to protein structure    prediction



l  h ihrig and  s  kambhampati 1997 storing and indexing plan derivations through explanationbased analysis of retrieval failures volume 7 pages 161198

casebased planning cbp provides a way of scaling up    domainindependent planning to solve large problems in complex    domains  it replaces the detailed and lengthy search for a solution    with the retrieval and adaptation of previous planning experiences    in general cbp has been demonstrated to improve performance over    generative fromscratch planning  however the performance    improvements it provides are dependent on adequate judgements as to    problem similarity  in particular although cbp may substantially    reduce planning effort overall it is subject to a misretrieval    problem the success of cbp depends on these retrieval errors being    relatively rare this paper describes the design and implementation of    a replay framework for the casebased planner dersnlpebl dersnlpebl    extends current cbp methodology by incorporating explanationbased    learning techniques that allow it to explain and learn from the    retrieval failures it encounters  these techniques are used to refine    judgements about case similarity in response to feedback when a wrong    decision has been made  the same failure analysis is used in building    the case library through the addition of repairing cases large    problems are split and stored as single goal subproblems  multigoal    problems are stored only when these smaller cases fail to be merged    into a full solution  an empirical evaluation of this approach    demonstrates the advantage of learning from experienced retrieval    failure



n  l zhang and  w  liu 1997 a model approximation scheme for planning in partially observable stochastic domains volume 7 pages 199230

partially observable markov decision processes pomdps are    a natural model for planning problems where effects of actions are    nondeterministic and the state of the world is not completely    observable  it is difficult to solve pomdps exactly  this paper    proposes a new approximation scheme  the basic idea is to transform a    pomdp into another one where additional information is provided by an    oracle the oracle informs the planning agent that the current state    of the world is in a certain region  the transformed pomdp is    consequently said to be region observable it is easier to solve than    the original pomdp  we propose to solve the transformed pomdp and use    its optimal policy to construct an approximate policy for the original    pomdp  by controlling the amount of additional information that the    oracle provides it is possible to find a proper tradeoff between    computational time and approximation quality  in terms of algorithmic    contributions we study in details how to exploit region observability    in solving the transformed pomdp to facilitate the study we also    propose a new exact algorithm for general pomdps  the algorithm is    conceptually simple and yet is significantly more efficient than all    previous exact algorithms



d  monderer and  m  tennenholtz 1997 dynamic nonbayesian decision making volume 7 pages 231248

the model of a nonbayesian agent who faces a repeated game    with incomplete information against nature is an appropriate tool for    modeling general agentenvironment interactions  in such a model the    environment state controlled by nature may change arbitrarily and    the feedbackreward function is initially unknown the agent is not    bayesian that is he does not form a prior probability neither on the    state selection strategy of nature nor on his reward function  a    policy for the agent is a function which assigns an action to every    history of observations and actions  two basic feedback structures    are considered in one of them  the perfect monitoring case  the    agent is able to observe the previous environment state as part of his    feedback while in the other  the imperfect monitoring case  all    that is available to the agent is the reward obtained both of these    settings refer to partially observable processes where the current    environment state is unknown  our main result refers to the    competitive ratio criterion in the perfect monitoring case we prove    the existence of an efficient stochastic policy that ensures that the    competitive ratio is obtained at almost all stages with an arbitrarily    high probability where efficiency is measured in terms of rate of    convergence  it is further shown that such an optimal policy does not    exist in the imperfect monitoring case moreover it is proved that in    the perfect monitoring case there does not exist a deterministic    policy that satisfies our long run optimality criterion  in addition    we discuss the maxmin criterion and prove that a deterministic    efficient optimal strategy does exist in the imperfect monitoring case    under this criterion finally we show that our approach to longrun    optimality can be viewed as qualitative which distinguishes it from    previous work in this area



j  frank  p  cheeseman and  j  stutz 1997 when gravity fails local search topology volume 7 pages 249281

local search algorithms for combinatorial search problems    frequently encounter a sequence of states in which it is impossible to    improve the value of the objective function moves through these    regions called plateau moves dominate the time spent in local    search  we analyze and characterize plateaus for three different    classes of randomly generated boolean satisfiability problems  we    identify several interesting features of plateaus that impact the    performance of local search algorithms  we show that local minima    tend to be small but occasionally may be very large  we also show    that local minima can be escaped without unsatisfying a large number    of clauses but that systematically searching for an escape route may    be computationally expensive if the local minimum is large  we show    that plateaus with exits called benches tend to be much larger than    minima and that some benches have very few exit states which local    search can use to escape  we show that the solutions ie global    minima of randomly generated problem instances form clusters which    behave similarly to local minima  we revisit several enhancements of    local search algorithms and explain their performance in light of our    results  finally we discuss strategies for creating the next    generation of local search algorithms



h  kaindl and  g  kainz 1997 bidirectional heuristic search reconsidered volume 7 pages 283317

the assessment of bidirectional heuristic search has been    incorrect since it was first published more than a quarter of a    century ago  for quite a long time this search strategy did not    achieve the expected results and there was a major misunderstanding    about the reasons behind it  although there is still widespread    belief that bidirectional heuristic search is afflicted by the problem    of search frontiers passing each other we demonstrate that this    conjecture is wrong  based on this finding we present both a new    generic approach to bidirectional heuristic search and a new approach    to dynamically improving heuristic values that is feasible in    bidirectional search only  these approaches are put into perspective    with both the traditional and more recently proposed approaches in    order to facilitate a better overall understanding  empirical results    of experiments with our new approaches show that bidirectional    heuristic search can be performed very efficiently and also with    limited memory  these results suggest that bidirectional heuristic    search appears to be better for solving certain difficult problems    than corresponding unidirectional search  this provides some evidence    for the usefulness of a search strategy that was long neglected  in    summary we show that bidirectional heuristic search is viable and    consequently propose that it be reconsidered
