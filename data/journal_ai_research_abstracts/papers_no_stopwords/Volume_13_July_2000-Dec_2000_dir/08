2003 ijcaijair best prize

presents new hierarchical reinforcement learning decomposing target markov decision process mdp hierarchy smaller mdps decomposing value function target mdp additive combination value functions smaller mdps decomposition known maxq decomposition procedural semanticsas subroutine hierarchyand declarative semanticsas representation value function hierarchical policy maxq unifies extends previous work hierarchical reinforcement learning singh kaelbling dayan hinton assumption programmer identify useful subgoals define subtasks achieve subgoals defining subgoals programmer constrains set policies need considered reinforcement learning maxq value function decomposition represent value function policy consistent given hierarchy decomposition creates opportunities exploit state abstractions individual mdps within hierarchy ignore large parts state space important practical application method defines maxq hierarchy proves formal representational power establishes five conditions safe use state abstractions presents online modelfree learning maxqq proves converges probability 1 kind locallyoptimal policy known recursively optimal policy even presence five kinds state abstraction evaluates maxq representation maxqq series experiments three domains shows experimentally maxqq state abstractions converges recursively optimal policy much faster flat q learning fact maxq learns representation value function important benefit makes possible compute execute improved nonhierarchical policy via procedure similar policy improvement step policy iteration demonstrates effectiveness nonhierarchical execution experimentally finally concludes comparison related work discussion design tradeoffs hierarchical reinforcement learning

