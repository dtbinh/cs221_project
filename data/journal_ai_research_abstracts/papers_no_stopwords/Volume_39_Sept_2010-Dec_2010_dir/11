geist pietquin 2010 kalman temporal differences 39 483532

reinforcement learning suffers lack scalability online value q function approximation received increasing interest last decade contribution introduces novel approximation scheme namely kalman temporal differences ktd framework exhibits following features sampleefficiency nonlinear approximation nonstationarity handling uncertainty management first ktdbased provided deterministic markov decision processes mdp produces biased estimates case stochastic transitions extended ktd framework xktd solving stochastic mdp described convergence analyzed special cases deterministic stochastic transitions related experimented classical benchmarks compare favorably state art exhibiting announced features

