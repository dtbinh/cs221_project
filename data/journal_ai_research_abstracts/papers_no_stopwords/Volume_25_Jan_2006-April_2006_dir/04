honorable mention 2011 ijcaijair best prize

study policy selection large relational markov decision processes mdps consider variant approximate policy iteration api replaces usual valuefunction learning step learning step policy space advantageous domains good policies easier represent learn corresponding value functions often case relational mdps interested order apply api problems introduce relational policy language corresponding learner addition introduce new bootstrapping routine goalbased domains random walks bootstrapping necessary many large relational mdps reward extremely sparse api ineffective domains initialized uninformed policy experiments resulting system able good policies number classical domains stochastic variants solving extremely large relational mdps experiments point limitations suggesting future work

