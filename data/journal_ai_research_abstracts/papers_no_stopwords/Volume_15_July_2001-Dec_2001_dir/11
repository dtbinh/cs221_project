j baxter p l bartlett l weaver 2001 experiments infinitehorizon policygradient estimation 15 351381

present perform gradient ascent average reward partially observable markov decision process pomdp gpomdp introduced companion baxter bartlett computes biased estimates performance gradient pomdps chief advantages uses one free parameter beta natural interpretation terms biasvariance tradeoff requires knowledge underlying state applied infinite state control observation spaces gradient estimates produced gpomdp used perform gradient ascent traditional stochasticgradient conjugategradients utilizes gradient information bracket maxima line searches experimental presented illustrating theoretical baxter bartlett toy practical aspects number realistic problems

