j baxter p l bartlett 2001 infinitehorizon policygradient estimation 15 319350

gradientbased approaches direct policy search reinforcement learning received much recent attention means solve problems partial observability avoid problems associated policy degradation valuefunction methods introduce gpomdp simulationbased generating biased estimate gradient average reward partially observable markov decision processes pomdps controlled parameterized stochastic policies similar proposed kimura et al 1995 chief advantages requires storage twice number policy parameters uses one free beta natural interpretation terms biasvariance tradeoff requires knowledge underlying state prove convergence gpomdp correct choice parameter beta related mixing time controlled pomdp briefly describe extensions gpomdp controlled markov chains continuous state observation control spaces multipleagents higherorder derivatives version training stochastic policies internal states companion baxter et al gradient estimates generated gpomdp used traditional stochastic gradient conjugategradient procedure local optima average reward

