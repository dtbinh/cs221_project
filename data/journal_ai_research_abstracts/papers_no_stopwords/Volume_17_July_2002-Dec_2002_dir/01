e howe e dahlman 2002 critical assessment benchmark comparison 17 133

recent trends led empirical comparison becoming commonplace field started settle methodology comparisons obvious practical reasons requires running subset planners subset problems characterize methodology examine eight implicit assumptions problems planners metrics used many comparisons assumptions pr1 performance general purpose planner penalizedbiased executed sampling problems domains pr2 minor syntactic differences representation affect performance pr3 problems solvable strips capable planners unless require adl planner assumptions pl1 latest version planner best one use pl2 default parameter settings approximate good performance pl3 time cutoffs unduly bias outcome metrics assumptions m1 performance degrades similarly planner run degraded runtime environments eg machine platform m2 number plan steps distinguishes performance assumptions supported empirically particular planners affected differently assumptions conclude call community devote resources improving state practice especially enhancing available benchmark problems

